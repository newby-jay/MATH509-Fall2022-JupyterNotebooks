{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"S-D3dDUbKYwD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638896328606,"user_tz":420,"elapsed":163,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"ce0a0fe9-9787-4721-de1c-d78494e297a5"},"source":["%pylab inline\n","%config InlineBackend.figure_format = 'retina'\n","from ipywidgets import interact"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"markdown","metadata":{"id":"JqhCThoCkT25"},"source":["# A simple example of a rudimentary autodifferentiation package\n","I used some advanced features in this notebook: classes, inheritance, and generators. I tried to comment and include some additional examples (see Extra Notes at the end)\n","\n","**Warning: this is just an experimntal example. In practice, we will use TensorFlow for tasks that involve automatic differentiation.**"]},{"cell_type":"code","metadata":{"id":"oiblaVN9KYwG"},"source":["class Autodiff_Node(object): \n","    ## A class is a recipe for creating objects (with methods and atributes).\n","    ## This is called a 'base class', which is like a boiler plate recipe that \n","    ## many other classes will use a starting point, each making specific \n","    ## changes.\n","\n","\n","    ## All methods (unless otherwise specified) must have the first argument\n","    ## a variable called `self`, which is a copy of the object itself. Hence,\n","    ## one can access any method or atribute in the object throught the `self`\n","    ## variable.\n","    def __init__(self, parents): \n","        \"\"\"Parameters:\n","        ---------------\n","        `parents` a list of `Autodiff_Node` objects corresponding to the graph\n","            parents.\"\"\"\n","        ## initializer gets called once when you create (or instantiate) an \n","        ## object\n","        self._set_parents(parents)\n","        self._output_data = None\n","    def _set_parents(self, parents):\n","        self.parents = parents\n","        return None\n","    def set_output_data(self, y):\n","        self._output_data = y\n","        return None\n","    def get_output_data(self):\n","        return self._output_data\n","    ## a static modthod just means it doesn't depend on the data in `self`, so \n","    ## `self` does not need to be an argument\n","    @staticmethod \n","    def function(x): \n","        \"\"\"Given input `x` return output `y`\"\"\"\n","        ## this is just a place holder (or template) to be used to create \n","        ## specific types of Node objects\n","        return NotImplementedError\n","    ## a static modthod just means it doesn't depend on the data in `self`, so \n","    ## `self` does not need to be an argument\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient): \n","        \"\"\"\n","        Parameters:\n","        --------------------\n","        `x` is the input variable(s): a list of tensors one for each input from \n","            a graph parent.\n","        `y` is the output variable(s): a list of tensors one for each ouput to \n","            a graph child.\n","        `output_gradient` is the gradient (list of partial derivatives) of a \n","            scalar function with respect to one or more output variables.\n","        \n","        Returns:\n","        --------------------\n","        `input_gradient` is the gradient (list of partial derivatives) of a \n","            scalar function with respect to one or more input variables.\"\"\"\n","        ## this is just a place holder (or template) to be used to create \n","        ## specific types of Node objects\n","        return NotImplementedError\n","    def eval(self):\n","        \"\"\"Evaluate the output of the node, moving from necessary inputs \n","        through the DAG in the forward direction.\"\"\"\n","        ## recursively call eval for each node until input variables are reached\n","        x = [node.eval() for node in self.parents] \n","        return self.function(x)\n","    def _eval_and_save_output(self):\n","        ## this is a stateful approach and should be used with care. This method \n","        ## will alter one of the atributes. This can lead to confusing and hard \n","        ## to diagnose bugs. It is best to avoid doing this whenever possible.\n","\n","        ## recursively call eval for each node until inputs are reached\n","        x = [node._eval_and_save_output() for node in self.parents]\n","        y = self.function(x)\n","        ## internal data, or state, is modified here. Specifically the \n","        ## `self._output_data` attribute.\n","        self.set_output_data(y) \n","        return y\n","    def _get_gradient(self, output_gradient):\n","        ## This is a helper function to assemble the gradients, moving backward \n","        ## through the DAG. We must call `_eval_and_save_output()` before \n","        ## using this method\n","        x = [node.get_output_data() for node in self.parents]\n","        ## We use internal state here, which assumes that \n","        ## `_eval_and_save_output()` was called before using this method\n","        y = self.get_output_data() \n","        input_gradient = self.backpropagation_function(x, y, output_gradient)\n","        ## We use recursion combined with generators (see examples at the end of \n","        ## this notebook)\n","        for node, sub_gradient in zip(self.parents, input_gradient):\n","            ## recursive call to the same method attached to the parent nodes\n","            for inner_gradient in node._get_gradient(sub_gradient): \n","                yield inner_gradient\n","    def compute_gradient(self): \n","        \"\"\"Assumes the node has scalar output\"\"\"\n","        ## computing gradients is very simple with the `Autodiff_node` class\n","\n","        ## the dangerous stateful call must precede the gradient calculation\n","        self._eval_and_save_output() \n","        ## the input is always simply `1.0` because partial_L/partial_L = 1\n","        return [g for g in self._get_gradient(1.)] \n","    # def __add__(self, b):\n","    #     ## You can define the \"+\" operator (and other operators)\n","    #     a = self\n","    #     return Add(a, b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kvpj7KgUKtKK"},"source":["class Add(Autodiff_Node):\n","    \"\"\"Add two input nodes\"\"\"\n","    ## this defines a node type specifically for addition, it 'inherits' all \n","    ## of the methods and atributes from its base class, `Autodiff_Node`. Think\n","    ## of these as default methods. Any methods that are redefined here are used \n","    ## instead of the default methods from the base class\n","    def __init__(self, a, b):\n","        ## initializer gets called once when you create (or instantiate) an \n","        ## object\n","        parents = [a, b]\n","        super().__init__(parents) ## calls `__init__` method of the base class\n","    ## a static modthod just means it doesn't depend on the data in `self`, so \n","    ## `self` does not need to be an argument\n","    @staticmethod\n","    def function(x):\n","        a = x[0]\n","        b = x[1]\n","        return a + b\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        input_gradient = [output_gradient*1, output_gradient*1]\n","        return input_gradient\n","\n","class Multiply(Autodiff_Node):\n","    \"\"\"Multiply two input nodes\"\"\"\n","    def __init__(self, a, b):\n","        parents = [a, b]\n","        super().__init__(parents)\n","    @staticmethod\n","    def function(x):\n","        a = x[0]\n","        b = x[1]\n","        return a*b\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        a = x[0]\n","        b = x[1]\n","        input_gradient = [output_gradient*b, output_gradient*a]\n","        return input_gradient\n","\n","class Tanh(Autodiff_Node):\n","    \"\"\"Apply the `tanh` function to an input node\"\"\"\n","    def __init__(self, x):\n","        parents = [x]\n","        super().__init__(parents)\n","    @staticmethod\n","    def function(x):\n","        return np.tanh(x[0])\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        dydx = 1./np.cosh(x[0])**2\n","        input_gradient = [output_gradient*dydx]\n","        return input_gradient\n","\n","class Input_Variable(Autodiff_Node):\n","    \"\"\"Input Variables have a specific fixed value. Use these to hold parameters \n","    and variables. Gradient of a node with a scalar output will be a list of \n","    partial derivatives with respect to these Input Variables.\n","    \n","    Parameters:\n","    ---------------\n","    `value` the numerical value of the variable (scalar in this example).\"\"\"\n","    def __init__(self, value):\n","        self.value = value\n","        parents = []\n","        super().__init__(parents)\n","    @staticmethod\n","    def function(x):\n","        return self.value\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        input_gradient = output_gradient\n","        return input_gradient\n","    def eval(self): \n","        ## this overrides the default `eval` method defined in `Autodiff_Node`\n","        ## base class\n","        return self.value\n","    def _eval_and_save_output(self): ## another override\n","        self.set_output_data(self.value)\n","        return self.value\n","    def _get_gradient(self, output_gradient): ## another override\n","        yield output_gradient"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TMX-1uAijCTj"},"source":["# Simple example"]},{"cell_type":"code","metadata":{"id":"NVnhB3YMKtFE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638896329888,"user_tz":420,"elapsed":141,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"1de58987-ad05-4999-d936-82bbfdecfc3e"},"source":["w = Input_Variable(1.2)\n","u = Input_Variable(2.)\n","b = Input_Variable(-3.)\n","\n","s1 = Multiply(w, u)\n","s2 = Add(s1, b)\n","\n","L = Tanh(s2)\n","\n","L.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.5370495669980354"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"XvdlyTHJKtH4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638896330897,"user_tz":420,"elapsed":138,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"4c6fc092-066f-402c-d8f1-c6ba51dfd9c6"},"source":["L.compute_gradient()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.4231555251744454, 0.8538933151046673, 0.7115777625872227]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"OVFMapxVKs_U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638896332514,"user_tz":420,"elapsed":271,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"3e4b3ded-3d3b-4b62-9632-9e4ec4f997f1"},"source":["## exact gradient for comparison\n","_g1 = 1./cosh(w.eval()*u.eval() + b.eval())**2\n","print('gradient (w, u, b):', _g1*u.eval(), _g1*w.eval(), _g1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["gradient (w, u, b): 1.4231555251744454 0.8538933151046673 0.7115777625872227\n"]}]},{"cell_type":"markdown","metadata":{"id":"PIqa5ysrj6Lv"},"source":["### We can evaluate at any of the nodes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bthffAGuj5u5","executionInfo":{"status":"ok","timestamp":1638896333978,"user_tz":420,"elapsed":121,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"bae2be7c-2f50-434d-fbe1-75d7fac6ab97"},"source":["s2.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.6000000000000001"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5azfWu5jj5m4","executionInfo":{"status":"ok","timestamp":1638896334531,"user_tz":420,"elapsed":4,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"4e11d174-0d3c-4d68-f6c7-8310736bbee4"},"source":["s2.compute_gradient()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2.0, 1.2, 1.0]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ug3ESbsqh-X2"},"source":["# What happens when an input variable is an input into more than one node?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"392h9vbSgvW4","executionInfo":{"status":"ok","timestamp":1638896335506,"user_tz":420,"elapsed":3,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"1578a01f-b4f9-4d30-e992-459228178682"},"source":["w = Input_Variable(1.2)\n","u = Input_Variable(2.)\n","b = Input_Variable(-10.)\n","\n","s1 = Multiply(w, u)\n","s2 = Add(s1, b)\n","\n","s3 = Multiply(s2, u)\n","\n","L = Tanh(s3)\n","\n","L.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.9999999999998745"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4H99yhe7iQXa","executionInfo":{"status":"ok","timestamp":1638896336346,"user_tz":420,"elapsed":2,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"0b8ff472-07af-486b-8986-01bb41b407bf"},"source":["L.compute_gradient() ## four outputs but we only have three input variables"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.0036163614810117e-12,\n"," 6.02169816888607e-13,\n"," 5.018081807405058e-13,\n"," -1.906871086813922e-12]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"pox1TfXAipyn"},"source":["### The resulting gradient has two components for the same variable\n","We probably just need to sum all of the elements of the output that correspond to the same input variable\n","-------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"axHzx2h-k7TF"},"source":["# Generalizations:\n","  1. We could generalize this to account for multidimensional array valued inputs and outputs (this might work in the current form for some operations). For example, suppose we want to do a matrix vector product. We wouldn't want to create a gigantic graph with all the little individual additions and multiplications. It would be far more efficient to define a new node type. We could call it `Matrix_Vector_Product` for example. \n","  2. We could also generalize so that we can use `if` statements, possibly even loops"]},{"cell_type":"markdown","metadata":{"id":"IWIEPPxqiR2W"},"source":["# Extra notes:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cS5ECGPZl6a4","executionInfo":{"status":"ok","timestamp":1638896342113,"user_tz":420,"elapsed":138,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"6cf031cb-1b79-4e8c-f4d4-9f6c250cc96b"},"source":["## a little generator example \n","[1 for j in arange(4)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jp7F-9dImAWt","executionInfo":{"status":"ok","timestamp":1638896342790,"user_tz":420,"elapsed":3,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"a5bda146-ff61-4962-bb19-0b6b0f656fed"},"source":["## a little generator example \n","[j for j in arange(4)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbGyJ_YvmAMc","executionInfo":{"status":"ok","timestamp":1638896342976,"user_tz":420,"elapsed":6,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"dd288216-e09b-4ad1-bb40-a4885be773f7"},"source":["## a little generator example \n","[[i for i in arange(j)] for j in arange(4)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[], [0], [0, 1], [0, 1, 2]]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88hmdZHG5N9L","executionInfo":{"status":"ok","timestamp":1638897580407,"user_tz":420,"elapsed":117,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"9a2a7cf9-33eb-4a1f-a7b8-792f0017e388"},"source":["## an advanced generator example using `yield` statements\n","def reverse_arange(n):\n","    for i in arange(n):\n","        yield n - 1 - i\n","    \n","[val for val in reverse_arange(3)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 1, 0]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYncH3eS5bFz","executionInfo":{"status":"ok","timestamp":1638897427725,"user_tz":420,"elapsed":112,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"26db0464-dbce-44ed-fda2-b8f3f21a4004"},"source":["## an advanced generator example using `yield` statements\n","def i1(n, m):\n","    for i in arange(n):\n","        for j in i2(m):\n","            yield j\n","def i2(n):\n","    for i in arange(n):\n","        yield i\n","\n","[val for val in i1(3, 2)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 0, 1, 0, 1]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Wy1wKAl5ogI","executionInfo":{"status":"ok","timestamp":1638897463107,"user_tz":420,"elapsed":120,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"279fd9f5-b817-441e-d93f-24fc2717699c"},"source":["[val for val in i1(2, 3)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 0, 1, 2]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"a6azXWr1Ks0y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638897667619,"user_tz":420,"elapsed":115,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"42fb2dda-3a51-4c51-a0f0-8e9e7a2bc7e9"},"source":["## an advanced generator example using `yield` statements\n","def i1(n, m):\n","    for i in arange(n):\n","        for j in i2(m):\n","            yield j\n","def i2(n):\n","    for i in arange(n):\n","        for j in i3():\n","            yield j\n","def i3():\n","    yield 5\n","    \n","[val for val in i1(3, 2)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5, 5, 5, 5, 5, 5]"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2CqQEUCi6Sp-","executionInfo":{"status":"ok","timestamp":1638897671018,"user_tz":420,"elapsed":120,"user":{"displayName":"Jay Newby","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKs0Q20tNydqtoa8lnP-bkOlAAhLnGzQ12qZt1=s64","userId":"15467205112857765885"}},"outputId":"9b6de7fe-7d1f-4cf6-f97f-eef38d6c27a9"},"source":["## an advanced generator example using `yield` statements\n","def i1(n, m):\n","    for i in arange(n):\n","        for j in i2(m):\n","            yield j\n","def i2(n):\n","    for i in arange(n):\n","        for j in i3():\n","            yield j\n","def i3():\n","    yield 5\n","    yield 3\n","\n","    \n","[val for val in i1(3, 2)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3]"]},"metadata":{},"execution_count":31}]}]}